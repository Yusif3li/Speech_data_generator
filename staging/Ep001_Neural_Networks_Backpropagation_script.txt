Speaker 1: أهلاً وسهلاً بيكم يا جماعة في حلقة جديدة من "تك توك"، بودكاستكم التقني المفضل. النهاردة عندنا موضوع مهم جداً وشيق، يمكن يكون معقد شوية بس هنحاول نبسطه بقدر الإمكان. هنتكلم عن حاجة أساسية في عالم الـ Artificial Intelligence وتحديداً الـ Neural Networks، وهي "Backpropagation". ومين أحسن من مهندسة برمجيات وخبرة في المجال ده عشان تشرح لنا؟ معانا النهاردة الأستاذة أحمد، مهندسة برمجيات متخصصة في الذكاء الاصطناعي. أهلاً بيكي يا أستاذة أحمد ومنورانا.

Speaker 2: أهلاً بيكي يا سارة، وشكراً جداً على الاستضافة والدعوة الكريمة. سعيدة إني أكون معاكم النهاردة.

Speaker 1: السعادة لينا طبعاً. بصي يا أستاذة أحمد، إحنا عارفين إن الـ Neural Networks دي حاجة بتشبه مخ الإنسان في طريقة التعلم، بس كتير من مستمعينا ممكن ميكونوش فاهمين إزاي المخ الصناعي ده بيتعلم بالظبط. ممكن تبدأي تشرحي لنا إيه هي الـ Neural Networks كده في جمل بسيطة كمدخل؟

Speaker 2: بالتأكيد يا سارة. ببساطة شديدة، الـ Neural Network هي عبارة عن Algorithm مستوحاة من طريقة عمل المخ البشري. بتتكون من مجموعة من الـ "Nodes" أو الخلايا العصبية الاصطناعية، متوصلة ببعضها في "Layers". كل Node في الـ Layer الواحدة بتاخد Input من الـ Nodes اللي قبلها، وتعمل عليها عمليات حسابية معينة، وبعدين تطلع Output. وده بيساعدها تفهم Patterns معقدة في الـ Data، زي مثلاً تمييز الوجوه أو فهم الكلام.

Speaker 1: تمام، يعني نقدر نقول إنها زي شبكة عصبية صناعية بتحاول تفهم المعلومات. طيب، إزاي الشبكة دي بتتعلم؟ يعني إحنا بنديها معلومات، وهي بتطلع لنا نتائج، ساعات بتكون صح وساعات غلط. إزاي بقى بتعرف إنها غلط وإزاي بتصحح أخطائها؟ هنا بقى بيجي دور الـ "Backpropagation" صح؟

Speaker 2: مظبوط جداً يا سارة، سؤالك في محله تماماً. الـ Neural Networks في البداية بتكون زي الطفل اللي لسه بيتعلم، بتطلع نتائج عشوائية أو غير دقيقة. الـ Backpropagation هي الـ Mechanism الأساسية اللي بتخليها تتعلم من أخطائها وتحسن من أدائها مع الوقت. تخيلي إننا بنعرض عليها صورة قطة، والشبكة بتطلع إنها "كلب". هنا حصل خطأ.

Speaker 1: اه، وبعدين بتعمل إيه عشان تصلح الغلط ده؟

Speaker 2: خليني أوضحلك الـ Process. أول حاجة بتحصل بنسميها "Forward Pass"؛ يعني الـ Data بتمشي في الشبكة من الـ Input Layer لحد الـ Output Layer، وبتطلع Prediction. بعد كده بنقارن الـ Prediction ده بالـ Actual Output، يعني بنشوف النتيجة اللي طلعتها الشبكة مقارنة بالإجابة الصحيحة. الفرق بينهم بنسميه "Error" أو "Loss".

Speaker 1: يعني بنحسب حجم الخطأ بتاعها.

Speaker 2: بالظبط. بعد ما بنحسب الـ Error ده، هنا بقى بيجي دور الـ "Backpropagation". الشبكة بترجع لورا، يعني بتتنقل من الـ Output Layer للـ Input Layer تاني، وبتحسب "Gradient" الـ Error ده لكل الـ "Weights" والـ "Biases" في الشبكة. الـ Weights والـ Biases دول هما الـ Parameters اللي الشبكة بتتعلمها. فـ Backpropagation بتعرفنا إزاي نغير الـ Weights والـ Biases دي عشان نقلل الـ Error ده في المرة الجاية.

Speaker 1: يعني كأنها بتشوف كل عنصر في الشبكة مسؤول عن قد إيه من الخطأ ده؟

Speaker 2: أيوه، وبتستخدم الـ Information دي عشان تعمل "Optimization" للـ Weights والـ Biases. بتعدلهم بقيم معينة عشان لما يجيلها نفس الـ Input تاني، الـ Error يقل والـ Prediction يكون أدق. وده بيحصل بشكل متكرر لآلاف أو ملايين المرات، كل مرة الشبكة بتتعلم وبتعدل الـ Parameters بتاعتها، لحد ما توصل لمرحلة يكون الـ Error فيها قليل جداً وبتطلع نتائج دقيقة. ده هو جوهر الـ Learning في الـ Deep Learning.

Speaker 1: يا سلام، يعني الموضوع ده هو اللي بيخلينا نشوف تطبيقات الـ AI الرهيبة اللي بنشوفها دلوقتي زي الـ Face Recognition في الموبايلات أو إن الـ Chatbots تفهم كلامنا كويس.

Speaker 2: تماماً يا سارة. الـ Backpropagation هي العمود الفقري اللي بني عليه كل التطورات اللي بنشوفها في الـ Artificial Intelligence، خصوصاً في مجالات زي الـ Computer Vision والـ Natural Language Processing. بدون الـ Algorithm دي، مكنش زماننا وصلنا للـ Capabilities اللي موجودة حالياً.

Speaker 1: بجد شرح وافي ومبسط بشكل ممتاز يا أستاذة أحمد. أنا متأكدة إن مستمعينا فهموا الموضوع ده اللي كان ممكن يبدو معقد في الأول. بشكرك جداً على وقتك وعلى المعلومات القيمة اللي قدمتيها لنا النهاردة.

Speaker 2: الشكر ليكي يا سارة وللمستمعين الكرام. كان شرف ليا أكون معاكم.

Speaker 1: الشرف لينا. وبكده نكون وصلنا لنهاية حلقة النهاردة من "تك توك". استنونا الأسبوع الجاي وموضوع تقني جديد. وإلى اللقاء!